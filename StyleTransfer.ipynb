{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hungarian-orleans",
   "metadata": {},
   "source": [
    "## Universal Style Transfer\n",
    "The models above are trained to work for a single style. Using these methods, in order to create a new style transfer model, you have to train the model with a wide variety of content images.\n",
    "\n",
    "Recent work by Yijun Li et al. shows that it is possible to create a model that generalizes to unseen style images, while maintaining the quality of output images.\n",
    "\n",
    "Their method works by treating style transfer as an image reconstruction task. They use the output of a VGG19 ReLU layer to encode features of various content images and traing a decoder to reconstruct these images. Then, with these two networks fixed, they feed the content and the style image into the encoder and use a whitening and coloring transform so that the covarience matrix of the features matches the covarience matrix of the style.\n",
    "\n",
    "This process can then be expanded to the remaining ReLU layers of VGG19 to create a style transfer pipeline that can apply to all spatial scales.\n",
    "\n",
    "Since only content images were used to train the encoder and decoder, additional training is not needed when generalizing this to new styles.\n",
    "\n",
    "<img src=\"images/universal-style-transfer.png\" style=\"width: 600px;\"/>\n",
    "(Yijun Li et al., Universal Style Transfer)\n",
    "\n",
    "<img src=\"images/doge_the_scream.jpg\" style=\"width: 300px;\"/>\n",
    "<img src=\"images/doge_mosaic.jpg\" style=\"width: 300px;\"/>\n",
    "\n",
    "The results are pretty impressive, but there are some patches of blurriness, most likely as a result of the transforms.\n",
    "\n",
    "### Whitening Transform\n",
    "\n",
    "The whitening transform removes the style from the content image, keeping the global content structure.\n",
    "\n",
    "The features of the content image, $f_c$, are transformed to obtain $\\hat{f}_c$, such that the feature maps\n",
    "are uncorrelated ($\\hat{f}_c \\hat{f}_c^T = I$),\n",
    "\n",
    "$$\n",
    "    \\hat{f}_c = E_c D_c^{- \\frac{1}{2}} E_c^T f_c\n",
    "$$\n",
    "\n",
    "where $D_c$ is a diagonal matrix with the eigenvalues of the covariance matrix $f_c f_c^T \\in R^{C \\times C}$,\n",
    "and $E_c$ is the corresponding orthogonal matrix of eigenvectors, satisfying $f_c f_c^T = E_c D_c E_c^T$.\n",
    "\n",
    "<img src=\"images/whitening.png\" style=\"width: 300px;\"/>\n",
    "(Yijun Li et al., Universal Style Transfer)\n",
    "\n",
    "\n",
    "### Coloring Transform\n",
    "\n",
    "The coloring transform adds the style from the style image onto the content image.\n",
    "\n",
    "The whitening transformed features of the content image, $\\hat{f}_c$, are transformed to obtain $\\hat{f}_{cs}$, such that the feature maps have that desired correlations ($\\hat{f}_{cs} \\hat{f}_{cs}^T = f_s f_s^T$),\n",
    "\n",
    "$$\n",
    "    \\hat{f}_{cs} = E_s D_s^{\\frac{1}{2}} E_s^T \\hat{f}_c\n",
    "$$\n",
    "\n",
    "where $D_s$ is a diagonal matrix with the eigenvalues of the covariance matrix $f_s f_s^T \\in R^{C \\times C}$,\n",
    "and $E_s$ is the corresponding orthogonal matrix of eigenvectors, satisfying $f_c f_c^T = E_c D_c E_c^T$.\n",
    "\n",
    "In practice, we also take a weighted sum of the colored and original activations such that:\n",
    "\n",
    "$$ f_{blend} = \\alpha\\hat{f}_{cs} + (1-\\alpha)\\hat{f}_c $$ \n",
    "\n",
    "Before each transform step, the mean of the corresponding feature maps are subtracted, and the mean of the style features are added back to the final transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nonprofit-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "# workaround for multiple OpenMP on Mac\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import PurePath\n",
    "\n",
    "import IPython.display as display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import time\n",
    "import functools\n",
    "print('here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specified-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img\n",
    "\n",
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title==None:\n",
    "        title = str(image.shape)\n",
    "    else:\n",
    "        title += ' '+str(image.shape)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-voice",
   "metadata": {},
   "source": [
    "# Using a pre-trained AutoEncoder\n",
    "For this assignment, i will be using an auto encoder created with Yihao Wang, a PhD student in the UbiComp lab here at SMU. The original code used to created this encoder is available for SMU students. \n",
    "\n",
    "The model that was trained can be downloaded from:\n",
    "https://www.dropbox.com/sh/2djb2c0ohxtvy2t/AAAxA2dnoFBcHGqfP0zLx-Oua?dl=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632dc78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 128, 128, 128)]   0         \n",
      "_________________________________________________________________\n",
      "decoder_block2_conv1 (Conv2D (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "decoder_block1_upsample (UpS (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "decoder_block1_conv2 (Conv2D (None, 256, 256, 64)      73792     \n",
      "_________________________________________________________________\n",
      "decoder_block1_conv1 (Conv2D (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "decoder_out (Conv2D)         (None, 256, 256, 3)       1731      \n",
      "=================================================================\n",
      "Total params: 260,035\n",
      "Trainable params: 260,035\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ModelBlock2 = tf.keras.models.load_model('decoder_2.h5', compile = False)\n",
    "ModelBlock2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "portable-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19AutoEncoder(tf.keras.Model):\n",
    "    def __init__(self, files_path):\n",
    "        super(VGG19AutoEncoder, self).__init__()\n",
    "        #Load Full Model with every trained decoder\n",
    "        \n",
    "        \n",
    "        #Get Each SubModel\n",
    "        # Each model has an encoder, a decoder, and an extra output convolution\n",
    "        # that converts the upsampled activations into output images\n",
    "        \n",
    "        # DO NOT load models four and five because they are not great auto encoders\n",
    "        # and therefore will cause weird artifacts when used for style transfer \n",
    "        \n",
    "        ModelBlock3 = tf.keras.models.load_model(str(PurePath(files_path, 'Block3_Model')), compile = False)\n",
    "        self.E3 = ModelBlock3.layers[0] # VGG encoder\n",
    "        self.D3 = ModelBlock3.layers[1] # Trained decoder from VGG\n",
    "        self.O3 = ModelBlock3.layers[2] # Conv layer to get to three channels, RGB image\n",
    "        \n",
    "        ModelBlock2 = tf.keras.models.load_model('decoder_2.h5', compile = False)\n",
    "        self.E2 = ModelBlock2.layers[0] # VGG encoder\n",
    "        self.D2 = ModelBlock2.layers[1] # Trained decoder from VGG\n",
    "        self.O2 = ModelBlock2.layers[2] # Conv layer to get to three channels, RGB image\n",
    "        \n",
    "        # no special decoder for this one becasue VGG first layer has\n",
    "        # no downsampling. So the decoder is just a convolution \n",
    "        ModelBlock1 = tf.keras.models.load_model(str(PurePath(files_path, 'Block1_Model')), compile = False)\n",
    "        self.E1 = ModelBlock1.layers[0] # VGG encoder, one layer\n",
    "        self.O1 = ModelBlock1.layers[1] # Conv layer to get to three channels, RGB image\n",
    "        \n",
    "\n",
    "    def call(self, image, alphas=None, training  = False):\n",
    "        # Input should be dictionary with 'style' and 'content' keys\n",
    "        # {'style':style_image, 'content':content_image}\n",
    "        # value in each should be a 4D Tensor,: (batch, i,j, channel)\n",
    "        \n",
    "        style_image = image['style']\n",
    "        content_image = image['content']\n",
    "        \n",
    "        output_dict = dict()\n",
    "        # this will be the output, where each value is a styled \n",
    "        # version of the image at layer 1, 2, and 3. So each key in the \n",
    "        # dictionary corresponds to layer1, layer2, and layer3.\n",
    "        # we also give back the reconstructed image from the auto encoder\n",
    "        # so each value in the dict is a tuple (styled, reconstructed)\n",
    "        \n",
    "        x = content_image\n",
    "        # choose covariance function\n",
    "        # covariance is more stable, but signal will work for very small images\n",
    "        wct = self.wct_from_cov \n",
    "        \n",
    "        if alphas==None:\n",
    "            alphas = {'layer3':0.6, \n",
    "                      'layer2':0.6, \n",
    "                      'layer1':0.6}\n",
    "        \n",
    "        # ------Layer 3----------\n",
    "        # apply whiten/color on layer 3 from the original image\n",
    "        # get activations\n",
    "        a_c = self.E3(tf.constant(x))\n",
    "        a_s = self.E3(tf.constant(style_image))\n",
    "        # swap grammian of activations, blended with original\n",
    "        x = wct(a_c.numpy(), a_s.numpy(), alpha=alphas['layer3'])\n",
    "        # decode the new style\n",
    "        x = self.O3(self.D3(x))\n",
    "        x = self.enhance_contrast(x)\n",
    "        # get reconstruction\n",
    "        reconst3 = self.O3(self.D3(self.E3(tf.constant(content_image))))\n",
    "        # save off the styled and reconstructed images for display\n",
    "        blended3 = tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "        reconst3 = tf.clip_by_value(tf.squeeze(reconst3), 0, 1)\n",
    "        output_dict['layer3'] = (blended3, reconst3)\n",
    "        \n",
    "        # ------Layer 2----------\n",
    "        # apply whiten/color on layer 2 from the already blended image\n",
    "        # get activations\n",
    "        a_c = self.E2(tf.constant(x))\n",
    "        a_s = self.E2(tf.constant(style_image))\n",
    "        # swap grammian of activations, blended with original\n",
    "        x = wct(a_c.numpy(),a_s.numpy(), alpha=alphas['layer2'])\n",
    "        # decode the new style\n",
    "        x = self.O2(self.D2(x))\n",
    "        x = self.enhance_contrast(x,1.3)\n",
    "        # get reconstruction\n",
    "        reconst2 = self.O2(self.D2(self.E2(tf.constant(content_image))))\n",
    "        # save off the styled and reconstructed images for display\n",
    "        blended2 = tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "        reconst2 = tf.clip_by_value(tf.squeeze(reconst2), 0, 1)\n",
    "        output_dict['layer2'] = (blended2, reconst2)\n",
    "        \n",
    "        # ------Layer 1----------\n",
    "        # apply whiten/color on layer 1 from the already blended image\n",
    "        # get activations\n",
    "        a_c = self.E1(tf.constant(x))\n",
    "        a_s = self.E1(tf.constant(style_image))\n",
    "        # swap grammian of activations, blended with original\n",
    "        x = wct(a_c.numpy(),a_s.numpy(), alpha=alphas['layer1'])\n",
    "        # decode the new style\n",
    "        x = self.O1(x)\n",
    "        x = self.enhance_contrast(x,1.2)\n",
    "        # get reconstruction\n",
    "        reconst1 = self.O1(self.E1(tf.constant(content_image)))\n",
    "        # save off the styled and reconstructed images for display\n",
    "        blended1 = tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "        reconst1 = tf.clip_by_value(tf.squeeze(reconst1), 0, 1)\n",
    "        output_dict['layer1'] = (blended1, reconst1)\n",
    "           \n",
    "        return output_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def enhance_contrast(image, factor=1.25):\n",
    "        return tf.image.adjust_contrast(image,factor)\n",
    "        \n",
    "    @staticmethod\n",
    "    def wct_from_cov(content, style, alpha=0.6, eps=1e-5):\n",
    "        '''\n",
    "        https://github.com/eridgd/WCT-TF/blob/master/ops.py\n",
    "           Perform Whiten-Color Transform on feature maps using numpy\n",
    "           See p.4 of the Universal Style Transfer paper for equations:\n",
    "           https://arxiv.org/pdf/1705.08086.pdf\n",
    "        '''\n",
    "        # 1xHxWxC -> CxHxW\n",
    "        content_t = np.transpose(np.squeeze(content), (2, 0, 1))\n",
    "        style_t = np.transpose(np.squeeze(style), (2, 0, 1))\n",
    "\n",
    "        # CxHxW -> CxH*W\n",
    "        content_flat = content_t.reshape(-1, content_t.shape[1]*content_t.shape[2])\n",
    "        style_flat = style_t.reshape(-1, style_t.shape[1]*style_t.shape[2])\n",
    "\n",
    "        # applt a threshold for only the largets eigen values\n",
    "        eigen_val_thresh = 1e-5\n",
    "        \n",
    "        # ===Whitening transform===\n",
    "        # 1. take mean of each channel\n",
    "        mc = content_flat.mean(axis=1, keepdims=True)\n",
    "        fc = content_flat - mc\n",
    "        # 2. get covariance of content, take SVD\n",
    "        cov_c = np.dot(fc, fc.T) / (content_t.shape[1]*content_t.shape[2] - 1)\n",
    "        Uc, Sc, _ = np.linalg.svd(cov_c)\n",
    "        # 3. truncate the SVD to only the largest eigen values\n",
    "        k_c = (Sc > eigen_val_thresh).sum()\n",
    "        Dc = np.diag((Sc[:k_c]+eps)**-0.5)\n",
    "        Uc = Uc[:,:k_c]\n",
    "        # 4. Now make a whitened content image\n",
    "        fc_white = (Uc @ Dc @ Uc.T) @ fc\n",
    "\n",
    "        # ===Coloring transform===\n",
    "        # 1. take mean of each channel\n",
    "        ms = style_flat.mean(axis=1, keepdims=True)\n",
    "        fs = style_flat - ms\n",
    "        # 2. get covariance of style, take SVD\n",
    "        cov_s = np.dot(fs, fs.T) / (style_t.shape[1]*style_t.shape[2] - 1)\n",
    "        Us, Ss, _ = np.linalg.svd(cov_s)\n",
    "        # 3. truncate the SVD to only the largest eigen values\n",
    "        k_s = (Ss > eigen_val_thresh).sum()\n",
    "        Ds = np.sqrt(np.diag(Ss[:k_s]+eps))\n",
    "        Us = Us[:,:k_s]\n",
    "        # 4. Now make a colored image that mixes the Grammian of the style\n",
    "        #   with the whitened content image\n",
    "        fcs_hat = (Us @ Ds @ Us.T) @ fc_white\n",
    "        fcs_hat = fcs_hat + ms # add style mean back to each channel\n",
    "\n",
    "        # Blend transform features with original features\n",
    "        blended = alpha*fcs_hat + (1 - alpha)*(content_flat) \n",
    "\n",
    "        # CxH*W -> CxHxW\n",
    "        blended = blended.reshape(content_t.shape)\n",
    "        # CxHxW -> 1xHxWxC\n",
    "        blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "        return np.float32(blended)\n",
    "\n",
    "    @staticmethod\n",
    "    def wct_from_signal(content, style, alpha=0.6 ):\n",
    "        # This uses a more computational SVD decomposition to get the Grammian\n",
    "        # to match. However, the numerical precision makes this totally fail\n",
    "        # if the activations are too large. \n",
    "        # This code is only for reference based on our discussion of WCT\n",
    "        \n",
    "        # 1xHxWxC -> CxHxW\n",
    "        content_t = np.transpose(np.squeeze(content), (2, 0, 1))\n",
    "        style_t = np.transpose(np.squeeze(style), (2, 0, 1))\n",
    "\n",
    "        # CxHxW -> Cx(H*W)\n",
    "        content_flat = content_t.reshape(-1, content_t.shape[1]*content_t.shape[2])\n",
    "        style_flat = style_t.reshape(-1, style_t.shape[1]*style_t.shape[2])\n",
    "\n",
    "        singular_val_thresh = 1e-3\n",
    "        #-------------------------------------------\n",
    "        # Whitening transform and Coloring transform\n",
    "        # 1. SVD of content signals\n",
    "        mc = content_flat.mean()\n",
    "        fc = content_flat - mc\n",
    "        Uc, Sc, Vc = np.linalg.svd(fc, full_matrices=False)\n",
    "        k_c = (Sc > singular_val_thresh).sum()\n",
    "        \n",
    "        # 2. SVD of style signals\n",
    "        ms = style_flat.mean()\n",
    "        fs = style_flat - ms\n",
    "        Us, Ss, Vs = np.linalg.svd(fs, full_matrices=False)\n",
    "        k_s = (Ss > singular_val_thresh).sum()\n",
    "        \n",
    "        k = min(k_s,k_c)\n",
    "\n",
    "        # Blend transform features with original features\n",
    "        fcs = (Us[:,:k] @ np.diag(Ss[:k]) @ Vc[:k,:]) + mc\n",
    "        blended = alpha*fcs + (1 - alpha)*(content_flat)\n",
    "\n",
    "        # CxH*W -> CxHxW\n",
    "        blended = blended.reshape(content_t.shape)\n",
    "        # CxHxW -> 1xHxWxC\n",
    "        blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "        return np.float32(blended)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thermal-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 18:40:07.869578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-01 18:40:08.168521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-01 18:40:08.169418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:07:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.683GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-04-01 18:40:08.169648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-01 18:40:08.171262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-01 18:40:08.172906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-01 18:40:08.173172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-01 18:40:08.174864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-01 18:40:08.175839: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-01 18:40:08.179494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-01 18:40:08.179666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-01 18:40:08.180633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-01 18:40:08.181464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2022-04-01 18:40:08.182313: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2022-04-01 18:40:08.208320: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3692970000 Hz\n",
      "2022-04-01 18:40:08.209056: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5597b6061e90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-04-01 18:40:08.209086: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-04-01 18:40:08.209341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-01 18:40:08.210243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:07:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.683GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-04-01 18:40:08.210291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-01 18:40:08.210308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-01 18:40:08.210324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-01 18:40:08.210339: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-01 18:40:08.210354: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-01 18:40:08.210368: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-01 18:40:08.210383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-01 18:40:08.210463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-01 18:40:08.211379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-01 18:40:08.212228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2022-04-01 18:40:08.212269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-01 18:40:08.280838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-01 18:40:08.280859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
      "2022-04-01 18:40:08.280864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
      "2022-04-01 18:40:08.281030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-01 18:40:08.281542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-01 18:40:08.282040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-01 18:40:08.282520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10486 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1)\n",
      "2022-04-01 18:40:08.283827: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5597b77bd040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-04-01 18:40:08.283838: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 s, sys: 461 ms, total: 1.63 s\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "AE = VGG19AutoEncoder('../VGGDecoderWeights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "promotional-campbell",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 18:40:15.318860: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at whole_file_read_ops.cc:116 : Not found: images/dallas_hall.jpg; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "images/dallas_hall.jpg; No such file or directory [Op:ReadFile]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2136/2080953085.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path_to_img)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmax_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv2022/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    552\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         return read_file_eager_fallback(\n\u001b[0;32m--> 554\u001b[0;31m             filename, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    555\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv2022/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    590\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001b[0;32m--> 592\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m    593\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv2022/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv2022/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: images/dallas_hall.jpg; No such file or directory [Op:ReadFile]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "content_path = 'images/dallas_hall.jpg'\n",
    "style_path = 'images/mosaic_style.png'\n",
    "\n",
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content_image,'Content')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style_image,'Style')\n",
    "\n",
    "tmp = {'style':style_image, \n",
    "       'content':content_image}\n",
    "\n",
    "alphas = {'layer3':0.8, 'layer2':0.6, 'layer1':0.6}\n",
    "decoded_images = AE(tmp, alphas=alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "later-sunrise",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'style_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2136/1226547723.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Style'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoded_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Styled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'style_image' is not defined"
     ]
    }
   ],
   "source": [
    "imshow(style_image,'Style')\n",
    "for layer in decoded_images.keys():\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    imshow(decoded_images[layer][0],'Styled')\n",
    "    plt.subplot(1,2,2)\n",
    "    imshow(decoded_images[layer][1],'Reconstructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-webster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-bottom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-glasgow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
